{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75c3aad",
   "metadata": {},
   "source": [
    "### Agentic ai \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e935099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv \n",
    "from rich import print\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34d2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm= ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c7e22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The Groq platform is a high-performance computing platform designed for artificial intelligence <span style=\"font-weight: bold\">(</span>AI<span style=\"font-weight: bold\">)</span> and machine \n",
       "learning <span style=\"font-weight: bold\">(</span>ML<span style=\"font-weight: bold\">)</span> workloads. Here's an overview:\n",
       "\n",
       "**What is Groq?**\n",
       "Groq is a startup company founded in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>, focused on developing a new type of computing platform that can \n",
       "efficiently handle complex AI and ML computations. The company was founded by Jonathan Ross, who previously worked \n",
       "at Google on the Tensor Processing Unit <span style=\"font-weight: bold\">(</span>TPU<span style=\"font-weight: bold\">)</span> project.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Tensor Processing**: Groq's platform is designed to accelerate tensor computations, which are the core of many\n",
       "AI and ML algorithms. Tensors are multi-dimensional arrays used to represent complex data structures.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **High-Performance Computing**: The Groq platform is optimized for high-performance computing, with a focus on \n",
       "low latency and high throughput.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Custom ASIC**: Groq has developed a custom Application-Specific Integrated Circuit <span style=\"font-weight: bold\">(</span>ASIC<span style=\"font-weight: bold\">)</span> designed \n",
       "specifically for AI and ML workloads. This ASIC provides a significant boost in performance and efficiency compared\n",
       "to traditional CPUs and GPUs.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Software Framework**: Groq provides a software framework that allows developers to easily integrate their AI \n",
       "and ML models with the platform.\n",
       "\n",
       "**How does it work?**\n",
       "The Groq platform consists of several components:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Groq Chip**: The custom ASIC chip is the core of the platform, providing the high-performance computing \n",
       "capabilities.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Groq Card**: The Groq Card is a PCIe card that contains the Groq Chip and provides a interface to the host \n",
       "system.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Software Framework**: The software framework provides a set of APIs and tools that allow developers to compile\n",
       "and run their AI and ML models on the Groq platform.\n",
       "\n",
       "**Benefits:**\n",
       "The Groq platform offers several benefits, including:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **High Performance**: The platform provides significant performance improvements for AI and ML workloads, \n",
       "reducing latency and increasing throughput.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Low Power Consumption**: The custom ASIC design and optimized software framework result in lower power \n",
       "consumption compared to traditional computing platforms.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Easy Integration**: The software framework makes it easy for developers to integrate their AI and ML models \n",
       "with the Groq platform.\n",
       "\n",
       "**Target Markets:**\n",
       "The Groq platform is designed for various markets, including:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Data Centers**: The platform is optimized for data center deployments, providing high-performance computing \n",
       "capabilities for cloud-based AI and ML services.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Edge Computing**: The platform is also suitable for edge computing applications, such as autonomous vehicles, \n",
       "smart homes, and industrial automation.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Research and Development**: The Groq platform is used by researchers and developers to accelerate AI and ML \n",
       "research and development.\n",
       "\n",
       "Overall, the Groq platform is a powerful tool for accelerating AI and ML workloads, providing high-performance \n",
       "computing capabilities, low power consumption, and easy integration.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The Groq platform is a high-performance computing platform designed for artificial intelligence \u001b[1m(\u001b[0mAI\u001b[1m)\u001b[0m and machine \n",
       "learning \u001b[1m(\u001b[0mML\u001b[1m)\u001b[0m workloads. Here's an overview:\n",
       "\n",
       "**What is Groq?**\n",
       "Groq is a startup company founded in \u001b[1;36m2016\u001b[0m, focused on developing a new type of computing platform that can \n",
       "efficiently handle complex AI and ML computations. The company was founded by Jonathan Ross, who previously worked \n",
       "at Google on the Tensor Processing Unit \u001b[1m(\u001b[0mTPU\u001b[1m)\u001b[0m project.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Tensor Processing**: Groq's platform is designed to accelerate tensor computations, which are the core of many\n",
       "AI and ML algorithms. Tensors are multi-dimensional arrays used to represent complex data structures.\n",
       "\u001b[1;36m2\u001b[0m. **High-Performance Computing**: The Groq platform is optimized for high-performance computing, with a focus on \n",
       "low latency and high throughput.\n",
       "\u001b[1;36m3\u001b[0m. **Custom ASIC**: Groq has developed a custom Application-Specific Integrated Circuit \u001b[1m(\u001b[0mASIC\u001b[1m)\u001b[0m designed \n",
       "specifically for AI and ML workloads. This ASIC provides a significant boost in performance and efficiency compared\n",
       "to traditional CPUs and GPUs.\n",
       "\u001b[1;36m4\u001b[0m. **Software Framework**: Groq provides a software framework that allows developers to easily integrate their AI \n",
       "and ML models with the platform.\n",
       "\n",
       "**How does it work?**\n",
       "The Groq platform consists of several components:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Groq Chip**: The custom ASIC chip is the core of the platform, providing the high-performance computing \n",
       "capabilities.\n",
       "\u001b[1;36m2\u001b[0m. **Groq Card**: The Groq Card is a PCIe card that contains the Groq Chip and provides a interface to the host \n",
       "system.\n",
       "\u001b[1;36m3\u001b[0m. **Software Framework**: The software framework provides a set of APIs and tools that allow developers to compile\n",
       "and run their AI and ML models on the Groq platform.\n",
       "\n",
       "**Benefits:**\n",
       "The Groq platform offers several benefits, including:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **High Performance**: The platform provides significant performance improvements for AI and ML workloads, \n",
       "reducing latency and increasing throughput.\n",
       "\u001b[1;36m2\u001b[0m. **Low Power Consumption**: The custom ASIC design and optimized software framework result in lower power \n",
       "consumption compared to traditional computing platforms.\n",
       "\u001b[1;36m3\u001b[0m. **Easy Integration**: The software framework makes it easy for developers to integrate their AI and ML models \n",
       "with the Groq platform.\n",
       "\n",
       "**Target Markets:**\n",
       "The Groq platform is designed for various markets, including:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Data Centers**: The platform is optimized for data center deployments, providing high-performance computing \n",
       "capabilities for cloud-based AI and ML services.\n",
       "\u001b[1;36m2\u001b[0m. **Edge Computing**: The platform is also suitable for edge computing applications, such as autonomous vehicles, \n",
       "smart homes, and industrial automation.\n",
       "\u001b[1;36m3\u001b[0m. **Research and Development**: The Groq platform is used by researchers and developers to accelerate AI and ML \n",
       "research and development.\n",
       "\n",
       "Overall, the Groq platform is a powerful tool for accelerating AI and ML workloads, providing high-performance \n",
       "computing capabilities, low power consumption, and easy integration.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(llm.invoke(\"hi explain me about Groq platform \").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691eff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4bdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\n",
    "    \"https://docs.langchain.com/oss/python/langchain/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/agents\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/structured-output\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e5d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123570d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list=[doc for sublist in docs for doc in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e542351",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "doc_splits=text_splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0a7841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/overview'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LangChain overview - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">UIObservabilitycloseOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">notes and migration guide.If you encounter any issues or have feedback,\"</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/overview'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LangChain overview - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'your code, see the release notes and migration guide.If you encounter any issues or have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/overview'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'LangChain overview - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"LangChain\u001b[0m\u001b[32m overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to \u001b[0m\n",
       "\u001b[32mbuild the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + \u001b[0m\n",
       "\u001b[32mLangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep \u001b[0m\n",
       "\u001b[32mAgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet \u001b[0m\n",
       "\u001b[32mstartedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term \u001b[0m\n",
       "\u001b[32mmemoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mMCP\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHuman-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat \u001b[0m\n",
       "\u001b[32mUIObservabilitycloseOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain\u001b[0m\n",
       "\u001b[32mv1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release \u001b[0m\n",
       "\u001b[32mnotes and migration guide.If you encounter any issues or have feedback,\"\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/overview'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'LangChain overview - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'your code, see the release notes and migration guide.If you encounter any issues or have \u001b[0m\n",
       "\u001b[32mfeedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(doc_splits[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c77f5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OllamaEmbeddings(model=\"llama3:8b\")\n",
    ")\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc57f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a5fb9026-6083-4ef6-a096-c0f711d50330'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/structured-output'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Structured output - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'If handle_errors is a tuple of exceptions, the agent will only retry (using the default error</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">message) if the exception raised is one of the specified types. In all other cases, the exception will be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">raised.\\nCustom error handler function:\\nCopyAsk AIdef custom_error_handler(error: Exception) -&gt; str:\\n    if </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">isinstance(error, StructuredOutputValidationError):\\n        return \"There was an issue with the format. Try </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">again.\\n    elif isinstance(error, MultipleStructuredOutputsError):\\n        return \"Multiple structured outputs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">were returned. Pick the most relevant one.\"\\n    else:\\n        return f\"Error: {str(error)}\"\\n\\nToolStrategy(\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">schema=ToolStrategy(Union[ContactInfo, EventDetails]),\\n    handle_errors=custom_error_handler\\n)\\n\\nOn </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">StructuredOutputValidationError:\\nCopyAsk AI================================= Tool Message </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">=================================\\nName: ToolStrategy\\n\\nThere was an issue with the format. Try again.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'b6b9228e-6c98-4bb7-8417-a3ee499e4eae'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/structured-output'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Structured output - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'strategiesCore componentsStructured outputCopy pageCopy pageStructured output allows agents </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'180c63fa-33d2-42b0-8997-c64bc952ebb2'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/agents'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Agents - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'agent = create_agent(\\n    model,\\n    tools=tools,\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">middleware=[CustomMiddleware()]\\n)\\n\\n# The agent can now track additional state beyond messages\\nresult = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\n\\u200bDefining state via </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state_schema\\nUse the state_schema parameter as a shortcut to define custom state that is only used in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tools.\\nCopyAsk AIfrom langchain.agents import AgentState\\n\\n\\nclass CustomState(AgentState):\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user_preferences: dict\\n\\nagent = create_agent(\\n    model,\\n    tools=[tool1, tool2],\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state_schema=CustomState\\n)\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"technical\", \"verbosity\": \"detailed\"},\\n})'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fd8cdb59-004b-4d68-a192-242a51560f45'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/structured-output'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Structured output - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'If handle_errors is a string, the agent will always prompt the model to re-try with a fixed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tool message:\\nCopyAsk AI================================= Tool Message =================================\\nName: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ProductRating\\n\\nPlease provide a valid rating between 1-5 and include a comment.\\n\\nHandle specific exceptions </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">only:\\nCopyAsk AIToolStrategy(\\n    schema=ProductRating,\\n    handle_errors=ValueError  # Only retry on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ValueError, raise others\\n)\\n\\nIf handle_errors is an exception type, the agent will only retry (using the default </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">error message) if the exception raised is the specified type. In all other cases, the exception will be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">raised.\\nHandle multiple exception types:\\nCopyAsk AIToolStrategy(\\n    schema=ProductRating,\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError\\n)'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'a5fb9026-6083-4ef6-a096-c0f711d50330'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/structured-output'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Structured output - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'If handle_errors is a tuple of exceptions, the agent will only retry \u001b[0m\u001b[32m(\u001b[0m\u001b[32musing the default error\u001b[0m\n",
       "\u001b[32mmessage\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if the exception raised is one of the specified types. In all other cases, the exception will be \u001b[0m\n",
       "\u001b[32mraised.\\nCustom error handler function:\\nCopyAsk AIdef custom_error_handler\u001b[0m\u001b[32m(\u001b[0m\u001b[32merror: Exception\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -> str:\\n    if \u001b[0m\n",
       "\u001b[32misinstance\u001b[0m\u001b[32m(\u001b[0m\u001b[32merror, StructuredOutputValidationError\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n        return \"There was an issue with the format. Try \u001b[0m\n",
       "\u001b[32magain.\\n    elif isinstance\u001b[0m\u001b[32m(\u001b[0m\u001b[32merror, MultipleStructuredOutputsError\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n        return \"Multiple structured outputs \u001b[0m\n",
       "\u001b[32mwere returned. Pick the most relevant one.\"\\n    else:\\n        return f\"Error: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mstr\u001b[0m\u001b[32m(\u001b[0m\u001b[32merror\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\\n\\nToolStrategy\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n   \u001b[0m\n",
       "\u001b[32mschema\u001b[0m\u001b[32m=\u001b[0m\u001b[32mToolStrategy\u001b[0m\u001b[32m(\u001b[0m\u001b[32mUnion\u001b[0m\u001b[32m[\u001b[0m\u001b[32mContactInfo, EventDetails\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32mhandle_errors\u001b[0m\u001b[32m=\u001b[0m\u001b[32mcustom_error_handler\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nOn \u001b[0m\n",
       "\u001b[32mStructuredOutputValidationError:\\nCopyAsk \u001b[0m\u001b[32mAI\u001b[0m\u001b[32m================================= Tool Message \u001b[0m\n",
       "\u001b[32m=================================\\nName: ToolStrategy\\n\\nThere was an issue with the format. Try again.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'b6b9228e-6c98-4bb7-8417-a3ee499e4eae'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/structured-output'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Structured output - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'strategiesCore componentsStructured outputCopy pageCopy pageStructured output allows agents \u001b[0m\n",
       "\u001b[32mto return data in a specific, predictable format. Instead of parsing natural language responses, you get structured\u001b[0m\n",
       "\u001b[32mdata in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'180c63fa-33d2-42b0-8997-c64bc952ebb2'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/agents'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Agents - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'agent = create_agent\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    model,\\n    \u001b[0m\u001b[32mtools\u001b[0m\u001b[32m=\u001b[0m\u001b[32mtools\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32mmiddleware\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32mCustomMiddleware\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# The agent can now track additional state beyond messages\\nresult = \u001b[0m\n",
       "\u001b[32magent.invoke\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    \"messages\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"role\": \"user\", \"content\": \"I prefer technical explanations\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32m\"user_preferences\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"style\": \"technical\", \"verbosity\": \"detailed\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\u200bDefining state via \u001b[0m\n",
       "\u001b[32mstate_schema\\nUse the state_schema parameter as a shortcut to define custom state that is only used in \u001b[0m\n",
       "\u001b[32mtools.\\nCopyAsk AIfrom langchain.agents import AgentState\\n\\n\\nclass CustomState\u001b[0m\u001b[32m(\u001b[0m\u001b[32mAgentState\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    \u001b[0m\n",
       "\u001b[32muser_preferences: dict\\n\\nagent = create_agent\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    model,\\n    \u001b[0m\u001b[32mtools\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32mtool1, tool2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32mstate_schema\u001b[0m\u001b[32m=\u001b[0m\u001b[32mCustomState\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n \u001b[0m\n",
       "\u001b[32m\"messages\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"role\": \"user\", \"content\": \"I prefer technical explanations\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\n    \"user_preferences\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"style\": \u001b[0m\n",
       "\u001b[32m\"technical\", \"verbosity\": \"detailed\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'fd8cdb59-004b-4d68-a192-242a51560f45'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/structured-output'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Structured output - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'If handle_errors is a string, the agent will always prompt the model to re-try with a fixed \u001b[0m\n",
       "\u001b[32mtool message:\\nCopyAsk \u001b[0m\u001b[32mAI\u001b[0m\u001b[32m================================= Tool Message =================================\\nName: \u001b[0m\n",
       "\u001b[32mProductRating\\n\\nPlease provide a valid rating between 1-5 and include a comment.\\n\\nHandle specific exceptions \u001b[0m\n",
       "\u001b[32monly:\\nCopyAsk AIToolStrategy\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32mschema\u001b[0m\u001b[32m=\u001b[0m\u001b[32mProductRating\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32mhandle_errors\u001b[0m\u001b[32m=\u001b[0m\u001b[32mValueError\u001b[0m\u001b[32m  # Only retry on \u001b[0m\n",
       "\u001b[32mValueError, raise others\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nIf handle_errors is an exception type, the agent will only retry \u001b[0m\u001b[32m(\u001b[0m\u001b[32musing the default \u001b[0m\n",
       "\u001b[32merror message\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if the exception raised is the specified type. In all other cases, the exception will be \u001b[0m\n",
       "\u001b[32mraised.\\nHandle multiple exception types:\\nCopyAsk AIToolStrategy\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32mschema\u001b[0m\u001b[32m=\u001b[0m\u001b[32mProductRating\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32mhandle_errors\u001b[0m\u001b[32m=\u001b[0m\u001b[32m(\u001b[0m\u001b[32mValueError, TypeError\u001b[0m\u001b[32m)\u001b[0m\u001b[32m  # Retry on ValueError and TypeError\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(retriever.invoke(\"what is Structured output ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427f3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3413ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"search and run information about langchain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "885c3c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Tool</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'retriever_vector_db_blog'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search and run information about langchain.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">args_schema</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'langchain_core.tools.retriever.RetrieverInput'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">func</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function _get_relevant_documents at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x0000025B4F241C60</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">retriever</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">VectorStoreRetriever</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">tags</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'FAISS'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'OllamaEmbeddings'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">vectorstore</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x0000025B4F2019A0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;, </span><span style=\"color: #808000; text-decoration-color: #808000\">search_kwargs</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{})</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">document_prompt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'page_content'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">input_types</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{}</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{}</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">template</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'{page_content}'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">document_separator</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\n'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">response_format</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">coroutine</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function _aget_relevant_documents at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x0000025B4F243740</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">retriever</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">VectorStoreRetriever</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">tags</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'FAISS'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'OllamaEmbeddings'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">vectorstore</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x0000025B4F2019A0</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">search_kwargs</span>=<span style=\"font-weight: bold\">{})</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">document_prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'page_content'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{page_content}'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">document_separator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\n'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">response_format</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'content'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTool\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mname\u001b[0m=\u001b[32m'retriever_vector_db_blog'\u001b[0m,\n",
       "    \u001b[33mdescription\u001b[0m=\u001b[32m'search and run information about langchain.'\u001b[0m,\n",
       "    \u001b[33margs_schema\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'langchain_core.tools.retriever.RetrieverInput'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mfunc\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m<function _get_relevant_documents at \u001b[0m\u001b[1;36m0x0000025B4F241C60\u001b[0m\u001b[39m>, \u001b[0m\n",
       "\u001b[33mretriever\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mVectorStoreRetriever\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mtags\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'FAISS'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'OllamaEmbeddings'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[33mvectorstore\u001b[0m\u001b[39m=<langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x0000025B4F2019A0\u001b[0m\u001b[39m>, \u001b[0m\u001b[33msearch_kwargs\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[33mdocument_prompt\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mPromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33minput_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'page_content'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33minput_types\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m, \u001b[0m\u001b[33mpartial_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[33mtemplate\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mpage_content\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, \u001b[0m\u001b[33mdocument_separator\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'\\n\\n'\u001b[0m\u001b[39m, \u001b[0m\u001b[33mresponse_format\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'content'\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mcoroutine\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m<function _aget_relevant_documents at \u001b[0m\u001b[1;36m0x0000025B4F243740\u001b[0m\u001b[39m>, \u001b[0m\n",
       "\u001b[33mretriever\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mVectorStoreRetriever\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mtags\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'FAISS'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'OllamaEmbeddings'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[33mvectorstore\u001b[0m\u001b[39m=<langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x0000025B4F2019A0\u001b[0m\u001b[1m>\u001b[0m, \u001b[33msearch_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m, \n",
       "\u001b[33mdocument_prompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'page_content'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[33mtemplate\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mpage_content\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mdocument_separator\u001b[0m=\u001b[32m'\\n\\n'\u001b[0m, \u001b[33mresponse_format\u001b[0m=\u001b[32m'content'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(retriever_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd8f5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## another vector database for langgraph \n",
    "\n",
    "urls=[\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e272e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[WebBaseLoader(doc).load() for doc in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb13b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list=[items for sublist in docs for items in sublist ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e571a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f06d3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits=text_splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98614523",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OllamaEmbeddings(model=\"llama3:8b\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5173c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67059ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool2=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog2\",\n",
    "    \"search and run information about langraph.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b75b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool,retriever_tool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1e26830",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph work flow \n",
    "\n",
    "from langgraph.graph import StateGraph,END,START\n",
    "from typing_extensions import TypedDict \n",
    "from typing import Annotated,Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages:Annotated[Sequence[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c2c506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59418b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"&lt;think&gt;\\nOkay, the user is asking about what Ollama is. I need to start by recalling what I know about</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Ollama. I remember that it's related to large language models, maybe something like LLaMA but open-source or a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">different approach. Let me think.\\n\\nOllama is a company that developed a tool or framework for running large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models locally. They have their own model called Ollama, which is based on LLaMA and Llama 2. The main </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">idea is to allow users to run these models on their own devices without needing a cloud service. That's useful for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">privacy and cost reasons.\\n\\nI should mention that Ollama provides a CLI and API for interacting with the models, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">making it easy to set up. They also support various models, not just their own, which adds versatility. The user </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">might be interested in how it compares to other tools like Docker or Colab. Perhaps they're looking to deploy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models locally for a project or personal use.\\n\\nWait, there's also the part about creating and sharing models. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Ollama allows users to train their own models or fine-tune existing ones. That's a key feature for customization. I</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">should highlight the ease of use, since Ollama is designed to be user-friendly, which is a big plus for developers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and researchers.\\n\\nI need to make sure I don't confuse Ollama with other projects like llama.cpp or other local </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model runners. Ollama is a specific product with its own ecosystem. Also, mentioning the open-source aspect might </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be important, but I should verify if their models are open-source. Llama 2 is open, but Ollama's models might be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">their own version.\\n\\nLet me structure this: start with a definition, then key features like local deployment, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">supported models, CLI/API, model creation, and use cases. Maybe add a note on performance and resource usage, but </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">I'm not sure about the exact specs. Also, check if there are any limitations, like hardware requirements for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">running models locally.\\n\\nThe user might be a developer or researcher looking for a way to test models locally, or</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">someone concerned about data privacy. They might want to know if Ollama is suitable for production use or just </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experimentation. I should keep the explanation clear and not too technical, but still cover the main points they'd </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">need to know.\\n&lt;/think&gt;\\n\\n**Ollama** is a local AI model runner and framework designed to simplify the deployment </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and use of large language models (LLMs) on personal devices or local servers. It enables users to run, manage, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interact with open-source LLMs like **Llama 2**, **Phi-2**, and others without relying on cloud services, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">emphasizing privacy, cost-effectiveness, and flexibility.\\n\\n---\\n\\n### **Key Features of Ollama**:\\n1. **Local </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Model Execution**:\\n   - Runs models directly on your machine (via GPU or CPU) without cloud dependency.\\n   - </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Supports popular open-source models like **Llama 2** and **Phi-2** out of the box.\\n\\n2. **Ease of Use**:\\n   - </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Provides a simple **command-line interface (CLI)** and **REST API** for setup and interaction.\\n   - No complex </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">configurations or infrastructure required.\\n\\n3. **Model Creation &amp; Sharing**:\\n   - Allows users to **train, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tune, or merge models** using tools like `ollama create`.\\n   - Models can be shared locally or distributed to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">others.\\n\\n4. **Cross-Platform Support**:\\n   - Works on **Windows, macOS, and Linux**.\\n   - Lightweight and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">optimized for performance, even on consumer-grade hardware.\\n\\n5. **Privacy-Focused**:\\n   - Data remains on your </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">device, avoiding the need to send it to external services.\\n\\n---\\n\\n### **How It Works**:\\n- **Download Models**: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Use `ollama pull` to fetch pre-trained models (e.g., `ollama pull llama2`).\\n- **Run Models**: Execute models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">locally via `ollama run` and interact with prompts.\\n- **Extend Capabilities**: Use the API to integrate models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">into applications or workflows.\\n\\n---\\n\\n### **Use Cases**:\\n- **Prototyping**: Quickly test AI ideas without </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cloud costs.\\n- **Privacy-Critical Tasks**: Handle sensitive data locally (e.g., internal company tools).\\n- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Educational Purposes**: Experiment with LLMs for learning or research.\\n- **Custom Applications**: Build </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">AI-powered tools tailored to specific needs (e.g., chatbots, code assistants).\\n\\n---\\n\\n### **Comparison to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Alternatives**:\\n- **llama.cpp**: A lower-level C/C++ project for running LLMs, requiring more technical setup.\\n- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Docker**: Ollama abstracts Docker complexity for easier model deployment.\\n- **Cloud Services (e.g., Hugging Face</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Inference API)**: Ollama avoids recurring costs and latency of cloud APIs.\\n\\n---\\n\\n### **Limitations**:\\n- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Requires sufficient **RAM (16GB+)** and a **GPU** (for faster performance) for larger models.\\n- Not all models are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">supported; focus is on open-source and well-optimized architectures.\\n\\n---\\n\\n### **Getting Started**:\\n1. Install</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Ollama from [ollama.ai](https://ollama.ai/).\\n2. Pull a model: `ollama pull llama2`.\\n3. Run and interact: `ollama </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">run llama2`.\\n\\nOllama is ideal for developers, researchers, and enthusiasts who want to leverage LLMs locally </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">without cloud overhead. Its simplicity and flexibility make it a popular choice for rapid experimentation and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">deployment.\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1141</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1155</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.844933438</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000513942</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'queue_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.052714298</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.84544738</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'qwen/qwen3-32b'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fp_5cf921caa2'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'service_tier'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'on_demand'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_provider'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'groq'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'lc_run--eaed2044-8f3a-4826-aa30-f3ffafc418c8-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1141</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1155</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m\"\u001b[0m\u001b[32m<\u001b[0m\u001b[32mthink\u001b[0m\u001b[32m>\\nOkay, the user is asking about what Ollama is. I need to start by recalling what I know about\u001b[0m\n",
       "\u001b[32mOllama. I remember that it's related to large language models, maybe something like LLaMA but open-source or a \u001b[0m\n",
       "\u001b[32mdifferent approach. Let me think.\\n\\nOllama is a company that developed a tool or framework for running large \u001b[0m\n",
       "\u001b[32mlanguage models locally. They have their own model called Ollama, which is based on LLaMA and Llama 2. The main \u001b[0m\n",
       "\u001b[32midea is to allow users to run these models on their own devices without needing a cloud service. That's useful for \u001b[0m\n",
       "\u001b[32mprivacy and cost reasons.\\n\\nI should mention that Ollama provides a CLI and API for interacting with the models, \u001b[0m\n",
       "\u001b[32mmaking it easy to set up. They also support various models, not just their own, which adds versatility. The user \u001b[0m\n",
       "\u001b[32mmight be interested in how it compares to other tools like Docker or Colab. Perhaps they're looking to deploy \u001b[0m\n",
       "\u001b[32mmodels locally for a project or personal use.\\n\\nWait, there's also the part about creating and sharing models. \u001b[0m\n",
       "\u001b[32mOllama allows users to train their own models or fine-tune existing ones. That's a key feature for customization. I\u001b[0m\n",
       "\u001b[32mshould highlight the ease of use, since Ollama is designed to be user-friendly, which is a big plus for developers \u001b[0m\n",
       "\u001b[32mand researchers.\\n\\nI need to make sure I don't confuse Ollama with other projects like llama.cpp or other local \u001b[0m\n",
       "\u001b[32mmodel runners. Ollama is a specific product with its own ecosystem. Also, mentioning the open-source aspect might \u001b[0m\n",
       "\u001b[32mbe important, but I should verify if their models are open-source. Llama 2 is open, but Ollama's models might be \u001b[0m\n",
       "\u001b[32mtheir own version.\\n\\nLet me structure this: start with a definition, then key features like local deployment, \u001b[0m\n",
       "\u001b[32msupported models, CLI/API, model creation, and use cases. Maybe add a note on performance and resource usage, but \u001b[0m\n",
       "\u001b[32mI'm not sure about the exact specs. Also, check if there are any limitations, like hardware requirements for \u001b[0m\n",
       "\u001b[32mrunning models locally.\\n\\nThe user might be a developer or researcher looking for a way to test models locally, or\u001b[0m\n",
       "\u001b[32msomeone concerned about data privacy. They might want to know if Ollama is suitable for production use or just \u001b[0m\n",
       "\u001b[32mexperimentation. I should keep the explanation clear and not too technical, but still cover the main points they'd \u001b[0m\n",
       "\u001b[32mneed to know.\\n</think\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\\n\\n**Ollama** is a local AI model runner and framework designed to simplify the deployment \u001b[0m\n",
       "\u001b[32mand use of large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m on personal devices or local servers. It enables users to run, manage, and \u001b[0m\n",
       "\u001b[32minteract with open-source LLMs like **Llama 2**, **Phi-2**, and others without relying on cloud services, \u001b[0m\n",
       "\u001b[32memphasizing privacy, cost-effectiveness, and flexibility.\\n\\n---\\n\\n### **Key Features of Ollama**:\\n1. **Local \u001b[0m\n",
       "\u001b[32mModel Execution**:\\n   - Runs models directly on your machine \u001b[0m\u001b[32m(\u001b[0m\u001b[32mvia GPU or CPU\u001b[0m\u001b[32m)\u001b[0m\u001b[32m without cloud dependency.\\n   - \u001b[0m\n",
       "\u001b[32mSupports popular open-source models like **Llama 2** and **Phi-2** out of the box.\\n\\n2. **Ease of Use**:\\n   - \u001b[0m\n",
       "\u001b[32mProvides a simple **command-line interface \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCLI\u001b[0m\u001b[32m)\u001b[0m\u001b[32m** and **REST API** for setup and interaction.\\n   - No complex \u001b[0m\n",
       "\u001b[32mconfigurations or infrastructure required.\\n\\n3. **Model Creation & Sharing**:\\n   - Allows users to **train, \u001b[0m\n",
       "\u001b[32mfine-tune, or merge models** using tools like `ollama create`.\\n   - Models can be shared locally or distributed to\u001b[0m\n",
       "\u001b[32mothers.\\n\\n4. **Cross-Platform Support**:\\n   - Works on **Windows, macOS, and Linux**.\\n   - Lightweight and \u001b[0m\n",
       "\u001b[32moptimized for performance, even on consumer-grade hardware.\\n\\n5. **Privacy-Focused**:\\n   - Data remains on your \u001b[0m\n",
       "\u001b[32mdevice, avoiding the need to send it to external services.\\n\\n---\\n\\n### **How It Works**:\\n- **Download Models**: \u001b[0m\n",
       "\u001b[32mUse `ollama pull` to fetch pre-trained models \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., `ollama pull llama2`\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n- **Run Models**: Execute models \u001b[0m\n",
       "\u001b[32mlocally via `ollama run` and interact with prompts.\\n- **Extend Capabilities**: Use the API to integrate models \u001b[0m\n",
       "\u001b[32minto applications or workflows.\\n\\n---\\n\\n### **Use Cases**:\\n- **Prototyping**: Quickly test AI ideas without \u001b[0m\n",
       "\u001b[32mcloud costs.\\n- **Privacy-Critical Tasks**: Handle sensitive data locally \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., internal company tools\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n- \u001b[0m\n",
       "\u001b[32m**Educational Purposes**: Experiment with LLMs for learning or research.\\n- **Custom Applications**: Build \u001b[0m\n",
       "\u001b[32mAI-powered tools tailored to specific needs \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., chatbots, code assistants\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n---\\n\\n### **Comparison to \u001b[0m\n",
       "\u001b[32mAlternatives**:\\n- **llama.cpp**: A lower-level C/C++ project for running LLMs, requiring more technical setup.\\n- \u001b[0m\n",
       "\u001b[32m**Docker**: Ollama abstracts Docker complexity for easier model deployment.\\n- **Cloud Services \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Hugging Face\u001b[0m\n",
       "\u001b[32mInference API\u001b[0m\u001b[32m)\u001b[0m\u001b[32m**: Ollama avoids recurring costs and latency of cloud APIs.\\n\\n---\\n\\n### **Limitations**:\\n- \u001b[0m\n",
       "\u001b[32mRequires sufficient **RAM \u001b[0m\u001b[32m(\u001b[0m\u001b[32m16GB+\u001b[0m\u001b[32m)\u001b[0m\u001b[32m** and a **GPU** \u001b[0m\u001b[32m(\u001b[0m\u001b[32mfor faster performance\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for larger models.\\n- Not all models are\u001b[0m\n",
       "\u001b[32msupported; focus is on open-source and well-optimized architectures.\\n\\n---\\n\\n### **Getting Started**:\\n1. Install\u001b[0m\n",
       "\u001b[32mOllama from \u001b[0m\u001b[32m[\u001b[0m\u001b[32mollama.ai\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://ollama.ai/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n2. Pull a model: `ollama pull llama2`.\\n3. Run and interact: `ollama \u001b[0m\n",
       "\u001b[32mrun llama2`.\\n\\nOllama is ideal for developers, researchers, and enthusiasts who want to leverage LLMs locally \u001b[0m\n",
       "\u001b[32mwithout cloud overhead. Its simplicity and flexibility make it a popular choice for rapid experimentation and \u001b[0m\n",
       "\u001b[32mdeployment.\"\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m1141\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m14\u001b[0m,\n",
       "            \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m1155\u001b[0m,\n",
       "            \u001b[32m'completion_time'\u001b[0m: \u001b[1;36m2.844933438\u001b[0m,\n",
       "            \u001b[32m'prompt_time'\u001b[0m: \u001b[1;36m0.000513942\u001b[0m,\n",
       "            \u001b[32m'queue_time'\u001b[0m: \u001b[1;36m0.052714298\u001b[0m,\n",
       "            \u001b[32m'total_time'\u001b[0m: \u001b[1;36m2.84544738\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'qwen/qwen3-32b'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m'fp_5cf921caa2'\u001b[0m,\n",
       "        \u001b[32m'service_tier'\u001b[0m: \u001b[32m'on_demand'\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'model_provider'\u001b[0m: \u001b[32m'groq'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'lc_run--eaed2044-8f3a-4826-aa30-f3ffafc418c8-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m14\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m1141\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m1155\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model.invoke(\"what is ollama?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a572b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\" \n",
    "    Invokes the agent model to generate a response based on the current state . Given the question,  it\n",
    "    will decide to retrieve using retriever tool or simply end.\n",
    "\n",
    "    Args : \n",
    "        state(messages) : the current state\n",
    "\n",
    "    returns: \n",
    "        dict : the updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"-- Call Agent -- \")\n",
    "    messages=state[\"messages\"]\n",
    "    llm= ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "    model=llm.bind_tools(tools)\n",
    "    response=model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5c06ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "from langchain_core.messages import BaseMessage,HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import Annotated,Literal,Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "002ee80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Edges \n",
    "\n",
    "def grade_documents(state)->Literal[\"generate\",\"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the queston.\n",
    "\n",
    "    Args : \n",
    "        str: A decision for whether the documents are relevant or not.\n",
    "    \n",
    "    Returns:\n",
    "        str : A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---Check relevance---\")\n",
    "\n",
    "    class grade(BaseModel):\n",
    "        \"\"\" Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM \n",
    "    llm= ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "    ## llm with tool and validation\n",
    "    llm_with_tools=llm.with_structured_output(grade)\n",
    "\n",
    "    # Prompt \n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\" \n",
    "                    you are a grader assessing relevance of a retrieved document to a\n",
    "                    user question. \\n\n",
    "                    here is the retrieved document: \\n\\n{context}\\n\\n\n",
    "                    here is the user question: {question} \\n\n",
    "                    if the documents contain keywords or semantic meaning related to the user question, grade it as relevant.\\n\n",
    "                    give binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "                \"\"\",\n",
    "                input_variables=[\"context\",\"question\"],\n",
    "    )\n",
    "    # Chain\n",
    "    chain=prompt | llm_with_tools\n",
    "\n",
    "    messages=state['messages']\n",
    "    last_message=messages[-1]\n",
    "\n",
    "    question=messages[0]\n",
    "    docs=last_message.content\n",
    "\n",
    "    scored_result=chain.invoke({'question':question,\"context\":docs})\n",
    "\n",
    "    score=scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---Decision : Docs relevant ---\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"---Decision : Docs are not relevant---\")\n",
    "        print(score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5019e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\" \n",
    "     Generate answer \n",
    "\n",
    "     Args : \n",
    "            state(message):the current state\n",
    "        returns: \n",
    "        dict: the updated message \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- generated---\")\n",
    "\n",
    "    messages=state[\"messages\"]\n",
    "    question=messages[0].content\n",
    "    last_message=messages[-1]\n",
    "    docs=last_message.content\n",
    "\n",
    "     # prompt \n",
    "    prompt=hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "     # llm \n",
    "    llm= ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "     # post-processing \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "\n",
    "    # Chain\n",
    "    rag_chain=prompt | llm | StrOutputParser()\n",
    "\n",
    "    # run \n",
    "    response=rag_chain.invoke({\"context\":docs,\"question\":question})\n",
    "    return {\"messages\":[response]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce2ba5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\" \n",
    "    Transform the query to produce a better quesion.\n",
    "\n",
    "    Args: \n",
    "        state (messages): the current state.\n",
    "\n",
    "    returns : \n",
    "            dict : the updated state with re-phrased question. \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---Transform Query---\")\n",
    "    messages=state['messages']\n",
    "    question=messages[0].content\n",
    "\n",
    "    msg=[\n",
    "\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\\n \n",
    "            look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "            Here is the initial question:\n",
    "            \\n------\\n\n",
    "            {question}\n",
    "            \\n-----\\n\n",
    "            Formulate an improved question: \n",
    "                \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader \n",
    "    model= ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "    response=model.invoke(msg)\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0abd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "\n",
    "builder=StateGraph(AgentState)\n",
    "\n",
    "# lets add nodes \n",
    "builder.add_node('agent',agent)\n",
    "retriever=ToolNode([retriever_tool,retriever_tool2])\n",
    "builder.add_node('retriever',retriever)\n",
    "builder.add_node(\"rewrite\",rewrite)\n",
    "builder.add_node(\"generate\",generate)\n",
    "\n",
    "# lets build workflow \n",
    "builder.add_edge(START,\"agent\")\n",
    "builder.add_conditional_edges(\"agent\",tools_condition,{\"tools\":\"retriever\",END:END,},)\n",
    "builder.add_conditional_edges(\"retriever\",grade_documents,{\"rewrite\":\"rewrite\",\"generate\":\"generate\"})\n",
    "builder.add_edge(\"generate\",END)\n",
    "builder.add_edge(\"rewrite\",\"agent\")\n",
    "\n",
    "\n",
    "graph=builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "669526ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display,Image\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png(output_file_path=\"output.png\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bfbd9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">-- Call Agent -- \n",
       "</pre>\n"
      ],
      "text/plain": [
       "-- Call Agent -- \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">---Check relevance---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "---Check relevance---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">---Decision : Docs relevant ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "---Decision : Docs relevant ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- generated---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- generated---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is langraph ? ', additional_kwargs={}, response_metadata={}, id='cf4a3099-f305-433c-92cd-20d3ed4766af'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '0krmm252c', 'function': {'arguments': '{\"query\":\"what is langraph\"}', 'name': 'retriever_vector_db_blog2'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 311, 'total_tokens': 333, 'completion_time': 0.036063638, 'prompt_time': 0.015944516, 'queue_time': 0.053385294, 'total_time': 0.052008154}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_0ca3f8c386', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--5b8dff7e-a85b-4f01-94cf-20cf0a8241d0-0', tool_calls=[{'name': 'retriever_vector_db_blog2', 'args': {'query': 'what is langraph'}, 'id': '0krmm252c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 311, 'output_tokens': 22, 'total_tokens': 333}),\n",
       "  ToolMessage(content=\"Break into discrete stepsEach node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps.State is shared memoryStore raw data, not formatted text. This lets different nodes use the same information in different ways.Nodes are functionsThey take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination.Errors are part of the flowTransient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging.Human input is first-classThe interrupt() function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.Graph structure emerges naturallyYou define the essential\\n\\nThinking in LangGraph - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedThinking in LangGraphLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimecloseOn this pageStart with the process you want to automateStep 1: Map out your workflow as discrete stepsStep 2: Identify what each step needs to doLLM StepsData StepsAction StepsUser Input StepsStep 3: Design your stateWhat belongs in state?Keep state raw, format prompts on-demandStep 4: Build your\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoRun a local serverPreviousWorkflows and agentsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat's new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\", name='retriever_vector_db_blog2', id='3b67b862-e15a-4386-a6f7-6d7fef655766', tool_call_id='0krmm252c'),\n",
       "  HumanMessage(content=\"LangGraph is a platform for agent engineering that enables the decomposition of processes into discrete steps, allowing for durable execution, streaming progress updates, and clear debugging. It is part of the LangChain ecosystem, which includes tools like LangSmith and provides APIs for building and managing workflows and agents. I don't know more specific details about its internal workings or implementation.\", additional_kwargs={}, response_metadata={}, id='67ac4581-c19c-469f-8d13-add03650171c')]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({'messages':\"What is langraph ? \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a30193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
