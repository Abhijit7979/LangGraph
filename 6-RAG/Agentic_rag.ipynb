{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75c3aad",
   "metadata": {},
   "source": [
    "### Agentic ai \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e935099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv \n",
    "from rich import print\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d34d2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm= ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2c7e22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The Groq platform is a relatively new and innovative AI computing platform designed to accelerate machine learning \n",
       "<span style=\"font-weight: bold\">(</span>ML<span style=\"font-weight: bold\">)</span> and artificial intelligence <span style=\"font-weight: bold\">(</span>AI<span style=\"font-weight: bold\">)</span> workloads. Here's an overview:\n",
       "\n",
       "**What is Groq?**\n",
       "Groq is a software-defined, high-performance computing platform that enables users to run ML and AI models at \n",
       "unprecedented speeds. The platform is designed to support a wide range of AI applications, including natural \n",
       "language processing, computer vision, and recommender systems.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Tensor Streaming Processor <span style=\"font-weight: bold\">(</span>TSP<span style=\"font-weight: bold\">)</span>**: Groq's proprietary TSP architecture is the core of the platform. It's a \n",
       "custom-designed processor that can handle complex tensor operations, which are the building blocks of most ML \n",
       "models.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Software-Defined**: The Groq platform is software-defined, which means that users can program and optimize the\n",
       "platform for their specific AI workloads using standard programming languages like Python and C++.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Scalability**: Groq is designed to scale horizontally, allowing users to add more processing units as needed \n",
       "to support large-scale AI deployments.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **High-Performance**: The platform is optimized for high-performance computing, with the ability to deliver \n",
       "petaflops <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> petaflop = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> million billion calculations per second<span style=\"font-weight: bold\">)</span> of performance.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. **Low Latency**: Groq's TSP architecture is designed to minimize latency, which is critical for real-time AI \n",
       "applications like autonomous vehicles, robotics, and video analytics.\n",
       "\n",
       "**Benefits:**\n",
       "The Groq platform offers several benefits, including:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Faster Model Training**: Groq can accelerate model training by up to 1<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x</span> compared to traditional GPU-based \n",
       "systems.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Improved Model Accuracy**: The platform's high-performance capabilities enable users to train larger, more \n",
       "complex models, leading to improved model accuracy.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Reduced Power Consumption**: Groq's software-defined architecture and TSP processor are designed to reduce \n",
       "power consumption, making it a more energy-efficient option for large-scale AI deployments.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Simplified Development**: The platform provides a simple, software-defined interface for developers, making it\n",
       "easier to integrate AI into existing applications and workflows.\n",
       "\n",
       "**Use Cases:**\n",
       "The Groq platform is suitable for a wide range of AI applications, including:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Autonomous Vehicles**: Groq can accelerate computer vision and sensor processing workloads for autonomous \n",
       "vehicles.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Natural Language Processing**: The platform can be used for large-scale language model training and inference.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Computer Vision**: Groq is suitable for image and video analysis, object detection, and segmentation.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Recommender Systems**: The platform can be used to build and deploy large-scale recommender systems.\n",
       "\n",
       "**Conclusion:**\n",
       "The Groq platform is an innovative AI computing platform that offers high-performance, low-latency, and \n",
       "software-defined capabilities for ML and AI workloads. Its benefits, including faster model training, improved \n",
       "model accuracy, and reduced power consumption, make it an attractive option for organizations looking to accelerate\n",
       "their AI initiatives.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The Groq platform is a relatively new and innovative AI computing platform designed to accelerate machine learning \n",
       "\u001b[1m(\u001b[0mML\u001b[1m)\u001b[0m and artificial intelligence \u001b[1m(\u001b[0mAI\u001b[1m)\u001b[0m workloads. Here's an overview:\n",
       "\n",
       "**What is Groq?**\n",
       "Groq is a software-defined, high-performance computing platform that enables users to run ML and AI models at \n",
       "unprecedented speeds. The platform is designed to support a wide range of AI applications, including natural \n",
       "language processing, computer vision, and recommender systems.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Tensor Streaming Processor \u001b[1m(\u001b[0mTSP\u001b[1m)\u001b[0m**: Groq's proprietary TSP architecture is the core of the platform. It's a \n",
       "custom-designed processor that can handle complex tensor operations, which are the building blocks of most ML \n",
       "models.\n",
       "\u001b[1;36m2\u001b[0m. **Software-Defined**: The Groq platform is software-defined, which means that users can program and optimize the\n",
       "platform for their specific AI workloads using standard programming languages like Python and C++.\n",
       "\u001b[1;36m3\u001b[0m. **Scalability**: Groq is designed to scale horizontally, allowing users to add more processing units as needed \n",
       "to support large-scale AI deployments.\n",
       "\u001b[1;36m4\u001b[0m. **High-Performance**: The platform is optimized for high-performance computing, with the ability to deliver \n",
       "petaflops \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m petaflop = \u001b[1;36m1\u001b[0m million billion calculations per second\u001b[1m)\u001b[0m of performance.\n",
       "\u001b[1;36m5\u001b[0m. **Low Latency**: Groq's TSP architecture is designed to minimize latency, which is critical for real-time AI \n",
       "applications like autonomous vehicles, robotics, and video analytics.\n",
       "\n",
       "**Benefits:**\n",
       "The Groq platform offers several benefits, including:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Faster Model Training**: Groq can accelerate model training by up to 1\u001b[1;36m0x\u001b[0m compared to traditional GPU-based \n",
       "systems.\n",
       "\u001b[1;36m2\u001b[0m. **Improved Model Accuracy**: The platform's high-performance capabilities enable users to train larger, more \n",
       "complex models, leading to improved model accuracy.\n",
       "\u001b[1;36m3\u001b[0m. **Reduced Power Consumption**: Groq's software-defined architecture and TSP processor are designed to reduce \n",
       "power consumption, making it a more energy-efficient option for large-scale AI deployments.\n",
       "\u001b[1;36m4\u001b[0m. **Simplified Development**: The platform provides a simple, software-defined interface for developers, making it\n",
       "easier to integrate AI into existing applications and workflows.\n",
       "\n",
       "**Use Cases:**\n",
       "The Groq platform is suitable for a wide range of AI applications, including:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Autonomous Vehicles**: Groq can accelerate computer vision and sensor processing workloads for autonomous \n",
       "vehicles.\n",
       "\u001b[1;36m2\u001b[0m. **Natural Language Processing**: The platform can be used for large-scale language model training and inference.\n",
       "\u001b[1;36m3\u001b[0m. **Computer Vision**: Groq is suitable for image and video analysis, object detection, and segmentation.\n",
       "\u001b[1;36m4\u001b[0m. **Recommender Systems**: The platform can be used to build and deploy large-scale recommender systems.\n",
       "\n",
       "**Conclusion:**\n",
       "The Groq platform is an innovative AI computing platform that offers high-performance, low-latency, and \n",
       "software-defined capabilities for ML and AI workloads. Its benefits, including faster model training, improved \n",
       "model accuracy, and reduced power consumption, make it an attractive option for organizations looking to accelerate\n",
       "their AI initiatives.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(llm.invoke(\"hi explain me about Groq platform \").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "691eff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d4bdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\n",
    "    \"https://docs.langchain.com/oss/python/langchain/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/agents\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/structured-output\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5e5d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "123570d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list=[doc for sublist in docs for doc in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e542351",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "doc_splits=text_splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c0a7841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/overview'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LangChain overview - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">UIObservabilitycloseOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">notes and migration guide.If you encounter any issues or have feedback,\"</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/overview'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LangChain overview - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'your code, see the release notes and migration guide.If you encounter any issues or have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/overview'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'LangChain overview - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"LangChain\u001b[0m\u001b[32m overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to \u001b[0m\n",
       "\u001b[32mbuild the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + \u001b[0m\n",
       "\u001b[32mLangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep \u001b[0m\n",
       "\u001b[32mAgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet \u001b[0m\n",
       "\u001b[32mstartedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term \u001b[0m\n",
       "\u001b[32mmemoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mMCP\u001b[0m\u001b[32m)\u001b[0m\u001b[32mHuman-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat \u001b[0m\n",
       "\u001b[32mUIObservabilitycloseOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain\u001b[0m\n",
       "\u001b[32mv1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release \u001b[0m\n",
       "\u001b[32mnotes and migration guide.If you encounter any issues or have feedback,\"\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/overview'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'LangChain overview - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'your code, see the release notes and migration guide.If you encounter any issues or have \u001b[0m\n",
       "\u001b[32mfeedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(doc_splits[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c77f5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OllamaEmbeddings(model=\"llama3:8b\")\n",
    ")\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebc57f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'f859446c-82d1-4045-9521-0507106c315a'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/structured-output'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Structured output - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'If handle_errors is a tuple of exceptions, the agent will only retry (using the default error</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">message) if the exception raised is one of the specified types. In all other cases, the exception will be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">raised.\\nCustom error handler function:\\nCopyAsk AIdef custom_error_handler(error: Exception) -&gt; str:\\n    if </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">isinstance(error, StructuredOutputValidationError):\\n        return \"There was an issue with the format. Try </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">again.\\n    elif isinstance(error, MultipleStructuredOutputsError):\\n        return \"Multiple structured outputs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">were returned. Pick the most relevant one.\"\\n    else:\\n        return f\"Error: {str(error)}\"\\n\\nToolStrategy(\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">schema=ToolStrategy(Union[ContactInfo, EventDetails]),\\n    handle_errors=custom_error_handler\\n)\\n\\nOn </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">StructuredOutputValidationError:\\nCopyAsk AI================================= Tool Message </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">=================================\\nName: ToolStrategy\\n\\nThere was an issue with the format. Try again.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'b20fd1d3-d40d-48b8-8126-702fe9e9e6da'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/structured-output'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Structured output - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'strategiesCore componentsStructured outputCopy pageCopy pageStructured output allows agents </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'8045d417-9a8a-471d-a178-e9e2de881642'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/agents'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Agents - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'agent = create_agent(\\n    model,\\n    tools=tools,\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">middleware=[CustomMiddleware()]\\n)\\n\\n# The agent can now track additional state beyond messages\\nresult = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\n\\u200bDefining state via </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state_schema\\nUse the state_schema parameter as a shortcut to define custom state that is only used in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tools.\\nCopyAsk AIfrom langchain.agents import AgentState\\n\\n\\nclass CustomState(AgentState):\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user_preferences: dict\\n\\nagent = create_agent(\\n    model,\\n    tools=[tool1, tool2],\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state_schema=CustomState\\n)\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"technical\", \"verbosity\": \"detailed\"},\\n})'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'d888fd5b-1db4-49b6-8eed-e33d25aa0bce'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://docs.langchain.com/oss/python/langchain/structured-output'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Structured output - Docs by LangChain'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'If handle_errors is a string, the agent will always prompt the model to re-try with a fixed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tool message:\\nCopyAsk AI================================= Tool Message =================================\\nName: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ProductRating\\n\\nPlease provide a valid rating between 1-5 and include a comment.\\n\\nHandle specific exceptions </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">only:\\nCopyAsk AIToolStrategy(\\n    schema=ProductRating,\\n    handle_errors=ValueError  # Only retry on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ValueError, raise others\\n)\\n\\nIf handle_errors is an exception type, the agent will only retry (using the default </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">error message) if the exception raised is the specified type. In all other cases, the exception will be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">raised.\\nHandle multiple exception types:\\nCopyAsk AIToolStrategy(\\n    schema=ProductRating,\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError\\n)'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'f859446c-82d1-4045-9521-0507106c315a'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/structured-output'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Structured output - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'If handle_errors is a tuple of exceptions, the agent will only retry \u001b[0m\u001b[32m(\u001b[0m\u001b[32musing the default error\u001b[0m\n",
       "\u001b[32mmessage\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if the exception raised is one of the specified types. In all other cases, the exception will be \u001b[0m\n",
       "\u001b[32mraised.\\nCustom error handler function:\\nCopyAsk AIdef custom_error_handler\u001b[0m\u001b[32m(\u001b[0m\u001b[32merror: Exception\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -> str:\\n    if \u001b[0m\n",
       "\u001b[32misinstance\u001b[0m\u001b[32m(\u001b[0m\u001b[32merror, StructuredOutputValidationError\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n        return \"There was an issue with the format. Try \u001b[0m\n",
       "\u001b[32magain.\\n    elif isinstance\u001b[0m\u001b[32m(\u001b[0m\u001b[32merror, MultipleStructuredOutputsError\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n        return \"Multiple structured outputs \u001b[0m\n",
       "\u001b[32mwere returned. Pick the most relevant one.\"\\n    else:\\n        return f\"Error: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mstr\u001b[0m\u001b[32m(\u001b[0m\u001b[32merror\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\\n\\nToolStrategy\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n   \u001b[0m\n",
       "\u001b[32mschema\u001b[0m\u001b[32m=\u001b[0m\u001b[32mToolStrategy\u001b[0m\u001b[32m(\u001b[0m\u001b[32mUnion\u001b[0m\u001b[32m[\u001b[0m\u001b[32mContactInfo, EventDetails\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32mhandle_errors\u001b[0m\u001b[32m=\u001b[0m\u001b[32mcustom_error_handler\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nOn \u001b[0m\n",
       "\u001b[32mStructuredOutputValidationError:\\nCopyAsk \u001b[0m\u001b[32mAI\u001b[0m\u001b[32m================================= Tool Message \u001b[0m\n",
       "\u001b[32m=================================\\nName: ToolStrategy\\n\\nThere was an issue with the format. Try again.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'b20fd1d3-d40d-48b8-8126-702fe9e9e6da'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/structured-output'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Structured output - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'strategiesCore componentsStructured outputCopy pageCopy pageStructured output allows agents \u001b[0m\n",
       "\u001b[32mto return data in a specific, predictable format. Instead of parsing natural language responses, you get structured\u001b[0m\n",
       "\u001b[32mdata in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'8045d417-9a8a-471d-a178-e9e2de881642'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/agents'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Agents - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'agent = create_agent\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    model,\\n    \u001b[0m\u001b[32mtools\u001b[0m\u001b[32m=\u001b[0m\u001b[32mtools\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32mmiddleware\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32mCustomMiddleware\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n# The agent can now track additional state beyond messages\\nresult = \u001b[0m\n",
       "\u001b[32magent.invoke\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    \"messages\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"role\": \"user\", \"content\": \"I prefer technical explanations\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32m\"user_preferences\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"style\": \"technical\", \"verbosity\": \"detailed\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\u200bDefining state via \u001b[0m\n",
       "\u001b[32mstate_schema\\nUse the state_schema parameter as a shortcut to define custom state that is only used in \u001b[0m\n",
       "\u001b[32mtools.\\nCopyAsk AIfrom langchain.agents import AgentState\\n\\n\\nclass CustomState\u001b[0m\u001b[32m(\u001b[0m\u001b[32mAgentState\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n    \u001b[0m\n",
       "\u001b[32muser_preferences: dict\\n\\nagent = create_agent\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    model,\\n    \u001b[0m\u001b[32mtools\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32mtool1, tool2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32mstate_schema\u001b[0m\u001b[32m=\u001b[0m\u001b[32mCustomState\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke\u001b[0m\u001b[32m(\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n \u001b[0m\n",
       "\u001b[32m\"messages\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"role\": \"user\", \"content\": \"I prefer technical explanations\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\n    \"user_preferences\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"style\": \u001b[0m\n",
       "\u001b[32m\"technical\", \"verbosity\": \"detailed\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'd888fd5b-1db4-49b6-8eed-e33d25aa0bce'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'https://docs.langchain.com/oss/python/langchain/structured-output'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Structured output - Docs by LangChain'\u001b[0m,\n",
       "            \u001b[32m'language'\u001b[0m: \u001b[32m'en'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'If handle_errors is a string, the agent will always prompt the model to re-try with a fixed \u001b[0m\n",
       "\u001b[32mtool message:\\nCopyAsk \u001b[0m\u001b[32mAI\u001b[0m\u001b[32m================================= Tool Message =================================\\nName: \u001b[0m\n",
       "\u001b[32mProductRating\\n\\nPlease provide a valid rating between 1-5 and include a comment.\\n\\nHandle specific exceptions \u001b[0m\n",
       "\u001b[32monly:\\nCopyAsk AIToolStrategy\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32mschema\u001b[0m\u001b[32m=\u001b[0m\u001b[32mProductRating\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32mhandle_errors\u001b[0m\u001b[32m=\u001b[0m\u001b[32mValueError\u001b[0m\u001b[32m  # Only retry on \u001b[0m\n",
       "\u001b[32mValueError, raise others\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nIf handle_errors is an exception type, the agent will only retry \u001b[0m\u001b[32m(\u001b[0m\u001b[32musing the default \u001b[0m\n",
       "\u001b[32merror message\u001b[0m\u001b[32m)\u001b[0m\u001b[32m if the exception raised is the specified type. In all other cases, the exception will be \u001b[0m\n",
       "\u001b[32mraised.\\nHandle multiple exception types:\\nCopyAsk AIToolStrategy\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32mschema\u001b[0m\u001b[32m=\u001b[0m\u001b[32mProductRating\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32mhandle_errors\u001b[0m\u001b[32m=\u001b[0m\u001b[32m(\u001b[0m\u001b[32mValueError, TypeError\u001b[0m\u001b[32m)\u001b[0m\u001b[32m  # Retry on ValueError and TypeError\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(retriever.invoke(\"what is Structured output ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427f3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3413ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"search and run information about langchain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "885c3c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Tool</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'retriever_vector_db_blog'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search and run information about langchain.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">args_schema</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'langchain_core.tools.retriever.RetrieverInput'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">func</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function _get_relevant_documents at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000001B06F348180</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">retriever</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">VectorStoreRetriever</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">tags</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'FAISS'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'OllamaEmbeddings'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">vectorstore</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000001B0CFADBA40</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;, </span><span style=\"color: #808000; text-decoration-color: #808000\">search_kwargs</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{})</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">document_prompt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'page_content'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">input_types</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{}</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{}</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">template</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'{page_content}'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">document_separator</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\n'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">response_format</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">coroutine</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function _aget_relevant_documents at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000001B06F348400</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">retriever</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">VectorStoreRetriever</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">tags</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'FAISS'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'OllamaEmbeddings'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">vectorstore</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000001B0CFADBA40</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">search_kwargs</span>=<span style=\"font-weight: bold\">{})</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">document_prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'page_content'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{page_content}'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">document_separator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\n'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">response_format</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'content'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTool\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mname\u001b[0m=\u001b[32m'retriever_vector_db_blog'\u001b[0m,\n",
       "    \u001b[33mdescription\u001b[0m=\u001b[32m'search and run information about langchain.'\u001b[0m,\n",
       "    \u001b[33margs_schema\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'langchain_core.tools.retriever.RetrieverInput'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mfunc\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m<function _get_relevant_documents at \u001b[0m\u001b[1;36m0x000001B06F348180\u001b[0m\u001b[39m>, \u001b[0m\n",
       "\u001b[33mretriever\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mVectorStoreRetriever\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mtags\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'FAISS'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'OllamaEmbeddings'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[33mvectorstore\u001b[0m\u001b[39m=<langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000001B0CFADBA40\u001b[0m\u001b[39m>, \u001b[0m\u001b[33msearch_kwargs\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[33mdocument_prompt\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mPromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33minput_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'page_content'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33minput_types\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m, \u001b[0m\u001b[33mpartial_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[33mtemplate\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mpage_content\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, \u001b[0m\u001b[33mdocument_separator\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'\\n\\n'\u001b[0m\u001b[39m, \u001b[0m\u001b[33mresponse_format\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'content'\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mcoroutine\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m<function _aget_relevant_documents at \u001b[0m\u001b[1;36m0x000001B06F348400\u001b[0m\u001b[39m>, \u001b[0m\n",
       "\u001b[33mretriever\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mVectorStoreRetriever\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mtags\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'FAISS'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'OllamaEmbeddings'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[33mvectorstore\u001b[0m\u001b[39m=<langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000001B0CFADBA40\u001b[0m\u001b[1m>\u001b[0m, \u001b[33msearch_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m, \n",
       "\u001b[33mdocument_prompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'page_content'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[33mtemplate\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mpage_content\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mdocument_separator\u001b[0m=\u001b[32m'\\n\\n'\u001b[0m, \u001b[33mresponse_format\u001b[0m=\u001b[32m'content'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(retriever_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd8f5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## another vector database for langgraph \n",
    "\n",
    "urls=[\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25e272e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[WebBaseLoader(doc).load() for doc in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb13b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list=[items for sublist in docs for items in sublist ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e571a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f06d3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits=text_splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98614523",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OllamaEmbeddings(model=\"llama3:8b\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5173c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67059ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool2=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog2\",\n",
    "    \"search and run information about langraph.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1b75b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool,retriever_tool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1e26830",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph work flow \n",
    "\n",
    "from langgraph.graph import StateGraph,END,START\n",
    "from typing_extensions import TypedDict \n",
    "from typing import Annotated,Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages:Annotated[Sequence[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c2c506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm=ChatOllama(model=\"llama3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59418b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Ollama!\n",
       "\n",
       "Ollama is a traditional Afro-Caribbean dish, specifically from Trinidad and Tobago. It's a popular one-pot meal \n",
       "made with a combination of ingredients like meat <span style=\"font-weight: bold\">(</span>usually beef or chicken<span style=\"font-weight: bold\">)</span>, potatoes, sweet potatoes, onions, bell \n",
       "peppers, tomatoes, Scotch bonnet peppers <span style=\"font-weight: bold\">(</span>which give it a unique flavor<span style=\"font-weight: bold\">)</span>, garlic, and herbs.\n",
       "\n",
       "The name <span style=\"color: #008000; text-decoration-color: #008000\">\"ollama\"</span> is believed to have originated from the African word <span style=\"color: #008000; text-decoration-color: #008000\">\"oli,\"</span> meaning <span style=\"color: #008000; text-decoration-color: #008000\">\"to mix together.\"</span> The dish \n",
       "is essentially a hearty stew that's slow-cooked in a large pot over an open flame or on a stovetop. The ingredients\n",
       "are simmered together until they're tender and flavorful, resulting in a rich, thick broth.\n",
       "\n",
       "Ollama is often served with rice, roti <span style=\"font-weight: bold\">(</span>a type of flatbread<span style=\"font-weight: bold\">)</span>, or as a filling for bread or dough. It's a beloved \n",
       "comfort food in Trinidadian culture, enjoyed by people of all ages.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Ollama!\n",
       "\n",
       "Ollama is a traditional Afro-Caribbean dish, specifically from Trinidad and Tobago. It's a popular one-pot meal \n",
       "made with a combination of ingredients like meat \u001b[1m(\u001b[0musually beef or chicken\u001b[1m)\u001b[0m, potatoes, sweet potatoes, onions, bell \n",
       "peppers, tomatoes, Scotch bonnet peppers \u001b[1m(\u001b[0mwhich give it a unique flavor\u001b[1m)\u001b[0m, garlic, and herbs.\n",
       "\n",
       "The name \u001b[32m\"ollama\"\u001b[0m is believed to have originated from the African word \u001b[32m\"oli,\"\u001b[0m meaning \u001b[32m\"to mix together.\"\u001b[0m The dish \n",
       "is essentially a hearty stew that's slow-cooked in a large pot over an open flame or on a stovetop. The ingredients\n",
       "are simmered together until they're tender and flavorful, resulting in a rich, thick broth.\n",
       "\n",
       "Ollama is often served with rice, roti \u001b[1m(\u001b[0ma type of flatbread\u001b[1m)\u001b[0m, or as a filling for bread or dough. It's a beloved \n",
       "comfort food in Trinidadian culture, enjoyed by people of all ages.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(llm.invoke(\"what is ollama\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a572b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\" \n",
    "    Invokes the agent model to generate a response based on the current state . Given the question,  it\n",
    "    will decide to retrieve using retriever tool or simply end.\n",
    "\n",
    "    Args : \n",
    "        state(messages) : the current state\n",
    "\n",
    "    returns: \n",
    "        dict : the updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"-- Call Agent -- \")\n",
    "    messages=state[\"messages\"]\n",
    "    model=llm.bind_tools(tools)\n",
    "    response=model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5c06ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "from langchain_core.messages import BaseMessage,HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import Annotated,Literal,Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "002ee80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Edges \n",
    "\n",
    "def grade_documents(state)->Literal[\"generate\",\"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the queston.\n",
    "\n",
    "    Args : \n",
    "        str: A decision for whether the documents are relevant or not.\n",
    "    \n",
    "    Returns:\n",
    "        str : A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---Check relevance---\")\n",
    "\n",
    "    class grade(BaseModel):\n",
    "        \"\"\" Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM \n",
    "    model=ChatOllama(model=\"llama3:8b\")\n",
    "\n",
    "    ## llm with tool and validation\n",
    "    llm_with_tools=model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt \n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\" \n",
    "                    you are a grader assessing relevance of a retrieved document to a\n",
    "                    user question. \\n\n",
    "                    here is the retrieved document: \\n\\n{context}\\n\\n\n",
    "                    here is the user question: {question} \\n\n",
    "                    if the documents contain keywords or semantic meaning related to the user question, grade it as relevant.\\n\n",
    "                    give binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "                \"\"\",\n",
    "                input_variables=[\"context\",\"question\"],\n",
    "    )\n",
    "    # Chain\n",
    "    chain=prompt | llm_with_tools\n",
    "\n",
    "    messages=state['messages']\n",
    "    last_message=messages[-1]\n",
    "\n",
    "    question=messages[0]\n",
    "    docs=last_message.content\n",
    "\n",
    "    scored_result=chain.invoke({'question':question,\"context\":docs})\n",
    "\n",
    "    score=scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---Decision : Docs relevant ---\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"---Decision : Docs are not relevant---\")\n",
    "        print(score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5019e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\" \n",
    "     Generate answer \n",
    "\n",
    "     Args : \n",
    "            state(message):the current state\n",
    "        returns: \n",
    "        dict: the updated message \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- generated---\")\n",
    "\n",
    "    messages=state[\"messages\"]\n",
    "    question=messages[0].content\n",
    "    last_message=messages[-1]\n",
    "    docs=last_message.content\n",
    "\n",
    "     # prompt \n",
    "    prompt=hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "     # llm \n",
    "    llm= ChatGroq(model=\"qwen-qwq-32b\")\n",
    "\n",
    "     # post-processing \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "\n",
    "    # Chain\n",
    "    rag_chain=prompt | llm | StrOutputParser()\n",
    "\n",
    "    # run \n",
    "    response=rag_chain.invoke({\"context\":docs,\"question\":question})\n",
    "    return {\"messages\":[response]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce2ba5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\" \n",
    "    Transform the query to produce a better quesion.\n",
    "\n",
    "    Args: \n",
    "        state (messages): the current state.\n",
    "\n",
    "    returns : \n",
    "            dict : the updated state with re-phrased question. \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---Transform Query---\")\n",
    "    messages=state['messages']\n",
    "    question=messages[0].content\n",
    "\n",
    "    msg=[\n",
    "\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\\n \n",
    "            look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "            Here is the initial question:\n",
    "            \\n------\\n\n",
    "            {question}\n",
    "            \\n-----\\n\n",
    "            Formulate an improved question: \n",
    "                \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader \n",
    "    model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "    response=model.invoke(msg)\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abd7a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph,START,END\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprebuilt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolNode,tools_condition\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m builder=StateGraph(\u001b[43mstate\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'state' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "\n",
    "builder=StateGraph(AgentState)\n",
    "\n",
    "# lets add nodes \n",
    "builder.add_node('agent',agent)\n",
    "retriever=ToolNode([retriever_tool,retriever_tool2])\n",
    "builder.add_node('retriever',retriever)\n",
    "builder.add_node(\"rewrite\",rewrite)\n",
    "builder.add_node(\"generate\",generate)\n",
    "\n",
    "# lets build workflow \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
